If you're new to #dataengineering or heard of big data analytics in #python, you probably have heard of #spark. But is it out of reach? I hope this example demonstrates how easy it can be. Moreover, if you've used #pandas, you're already a big data programmer, you just don't know it yet!

If you run `pip install pyspark`, you'll have all you need to start running a local instance. From there, your code can run in Hadoop, Databricks, or wherever Spark can run at scale. Then as this demo shows, you can take a local CSV file, define a schema like a SQL DDL expression or not, and be ready to start manipulating data or doing analytics. I'll have more examples of this in the future, so stay tuned!

Have you used Spark before? Do you run into problems processing Pandas or R dataframes in memory for large datasets? Have you explored Dask or DuckDB yet?

<img src="../../static/0007.png" />
